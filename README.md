<img src='https://imgur.com/9qkTE8Z.png'>

A place to keep track of all the annotated research papers. The aim of this repo is to house the annotated versions of trending/impactful research papers in machine learning. According to me the skill of reading research papers is very important for beginners and experienced folks alike. With a plethora of content out there in terms of blogs and jargon and complex terms(calling back to old papers, concepts, etc) the newer folks are simply moving away from it.

In order to make paper reading more accessible, I try to annotate, give insights, try to break some jargon on the paper itself, and carefully color-coding the highlights to distinguish the work already done vs the work proposed in the paper. This is my attempt to give back to the community in the tiniest of ways :D. Hope this is helpful to the people and helps in inculcating a habit of paper reading among all.


- ðŸ“« How to reach me **[@akshayuppal12](https://twitter.com/akshayuppal12)**
- ðŸ“„ Know about my experiences on [LinkedIn](https://www.linkedin.com/in/uppalakshay/)
- Check out my blog for such papers, tutorials and more: [https://au1206.github.io/](https://au1206.github.io/)

---


## Papers
| | Paper | Conference | Year |
| :---: | :--- | :---: | :---: |
|1.| [PICK : Processing Key Information Extraction from Documents using Improved Graph Learning-Convolutional Networks](https://github.com/au1206/paper_annotations/blob/master/PICK.pdf)|ICPR |2020|
|2.| [Attention is All you Need](https://github.com/au1206/paper_annotations/blob/master/attention_is_all_you_need.pdf) | NeurIPS |2017 |
|3.| [MLP-Mixer: An all MLP Architecture for Vision](https://github.com/au1206/paper_annotations/blob/master/mlp_mixer.pdf)| CVPR|May 2021|
|4.| [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://github.com/au1206/paper_annotations/blob/master/BERT.pdf)|NAACL 19|2018|
|5.| [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://github.com/au1206/paper_annotations/blob/master/EfficientNet.pdf)| ICML|2019|
|6.| [EfficientNetV2: Smaller Models and Faster Training](https://github.com/au1206/paper_annotations/blob/master/EfficientNet-v2.pdf) | ICML |2021|
|7.| [Few-Shot Named Entity Recognition: A Comprehensive Study](https://github.com/au1206/paper_annotations/blob/master/Few_shot_NER.pdf) | |Dec 2020|
|8.| [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://github.com/au1206/paper_annotations/blob/master/RoBERTa.pdf)| |Jul 2019 |
|9.| [LayoutLM: Pre-training of Text and Layout for Document Image Understanding](https://github.com/au1206/paper_annotations/blob/master/LayoutLM.pdf)| KDD 20 |Dec 2019|
|10.| [Fastformer: Additive Attention Can Be All You Need](https://github.com/au1206/paper_annotations/blob/master/Fastformer.pdf)||Sep 2021|
|11.| [LayoutLMv2: Multi-Modal Pre-Training For Visually-Rich Document Understanding](https://github.com/au1206/paper_annotations/blob/master/LayoutLMv2.pdf)|ACL|Sep 2021|
|12.| [WebFormer: The Web-page Transformer for Structure Information Extraction](https://github.com/au1206/paper_annotations/blob/master/Webformer.pdf) | WWW |Feb 2022|
|13.| [An Attention Free Transformer](https://github.com/au1206/paper_annotations/blob/master/an_attention_free_transformer.pdf)||Sep 2021|


---
## Sample Annotations
<img src="https://imgur.com/v1TnohA.gif" width='600'>


## Color Scheme
| Color | Meaning |
| :---: | :--- |
| Green | Topics about the current paper |
| Yellow | Topics about other relevant references |
| Blue | Implementation details/ maths/experiments |
| Red | Text including my thoughts, questions, and understandings |
---
